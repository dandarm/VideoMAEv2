{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistiche di valutazione del modello addestrato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    matthews_corrcoef,\n",
    "    cohen_kappa_score,\n",
    "    balanced_accuracy_score,\n",
    "    brier_score_loss,\n",
    "    log_loss\n",
    ")\n",
    "\n",
    "def evaluate_binary_classifier(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    y_score: Optional[np.ndarray] = None,   # ü†à qui\n",
    "    pos_label: Union[int, str] = 1,         # ü†à e qui\n",
    "    show_report: bool = True\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Restituisce un pandas Series con le metriche principali per la classificazione binaria.\n",
    "    Se passi anche y_score, calcola ROC-AUC, PR-AUC, log-loss e Brier score.\n",
    "    \"\"\"\n",
    "    # Controlli veloci\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    if y_score is not None:\n",
    "        y_score = np.asarray(y_score)\n",
    "\n",
    "    # Metriche base\n",
    "    metrics = {\n",
    "        \"accuracy\"              : accuracy_score(y_true, y_pred),\n",
    "        \"balanced_accuracy\"     : balanced_accuracy_score(y_true, y_pred),\n",
    "        \"precision\"             : precision_score(y_true, y_pred, pos_label=pos_label),\n",
    "        \"recall\"                : recall_score(y_true, y_pred, pos_label=pos_label),\n",
    "        \"f1\"                    : f1_score(y_true, y_pred, pos_label=pos_label),\n",
    "        \"matthews_corrcoef\"     : matthews_corrcoef(y_true, y_pred),\n",
    "        \"cohen_kappa\"           : cohen_kappa_score(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "    # Metriche che richiedono punteggi/ probabilit√†\n",
    "    if y_score is not None:\n",
    "        metrics.update({\n",
    "            \"roc_auc\"           : roc_auc_score(y_true, y_score),\n",
    "            \"pr_auc\"            : average_precision_score(y_true, y_score),\n",
    "            \"log_loss\"          : log_loss(y_true, y_score, labels=[0,1]),\n",
    "            \"brier_score_loss\"  : brier_score_loss(y_true, y_score),\n",
    "        })\n",
    "\n",
    "    # Confusion matrix ‚Äúflattened‚Äù\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, pos_label]).ravel()\n",
    "    metrics.update({\n",
    "        \"true_negatives\" : tn,\n",
    "        \"false_positives\": fp,\n",
    "        \"false_negatives\": fn,\n",
    "        \"true_positives\" : tp,\n",
    "    })\n",
    "\n",
    "    if show_report:\n",
    "        print(\"=== Confusion Matrix ===\")\n",
    "        print(confusion_matrix(y_true, y_pred, labels=[0, pos_label]))\n",
    "        print(f\"True Positive: {tp}\")\n",
    "        print(f\"True Negative: {tn}\")\n",
    "        print(f\"False Positive: {fp}\")\n",
    "        print(f\"False Negative: {fn}\")\n",
    "        print(\"\\n=== Classification Report ===\")\n",
    "        print(classification_report(y_true, y_pred, labels=[0, pos_label], target_names=[\"neg\", \"pos\"]))\n",
    "\n",
    "    return pd.Series(metrics, name=\"metrics\")\n",
    "\n",
    "# ESEMPIO D‚ÄôUSO -----------------------------------------------------------\n",
    "# y_true  = np.random.randint(0, 2, size=100)\n",
    "# y_pred  = np.random.randint(0, 2, size=100)\n",
    "# y_score = np.random.rand(100)\n",
    "# results = evaluate_binary_classifier(y_true, y_pred, y_score)\n",
    "# print(\"\\n=== Metriche ===\")\n",
    "# print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isac/miniconda3/envs/videomae/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of the class = 2\n",
      "Creating model: vit_giant_patch14_224 (nb_classes=2)\n",
      "Checkpoint loaded. Missing keys: []\n",
      "Checkpoint caricato correttamente.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from arguments import prepare_finetuning_args, Args\n",
    "from dataset import build_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import models\n",
    "from timm.models import create_model\n",
    "\n",
    "\n",
    "\n",
    "def predict_label(model, videos):\n",
    "    videos = videos.to(args.device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(videos)  # (B, nb_classes)\n",
    "        pred = torch.argmax(logits, dim=1)  # intero con l'indice di classe\n",
    "\n",
    "    pred_classes = pred.detach().cpu().numpy()\n",
    "\n",
    "    return pred_classes\n",
    "\n",
    "def get_path_pred_label(model, data_loader):\n",
    "    all_paths = []\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    for videos, labels, folder_path in data_loader:\n",
    "        predicted_classes = predict_label(model, videos) # shape (batch, num_class)\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        \n",
    "        all_labels.extend(labels)\n",
    "        all_preds.extend(predicted_classes)\n",
    "        all_paths.extend(folder_path)\n",
    "\n",
    "    return all_paths, all_preds, all_labels\n",
    "\n",
    "\n",
    "args = prepare_finetuning_args()\n",
    "##### prende da val_supervised.csv\n",
    "dataset_val, _ = build_dataset(is_train=False, test_mode=False, args=args)  \n",
    "\n",
    "data_loader_val = DataLoader(\n",
    "    dataset_val,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,         # Per estrarre sample casuali\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=args.pin_mem,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "get_prediction = False\n",
    "\n",
    "# voglio prendere le predizioni\n",
    "get_prediction = True\n",
    "\n",
    "# istanzia l'oggetto del modello \n",
    "print(f\"Creating model: {args.model} (nb_classes={args.nb_classes})\")\n",
    "model = create_model(\n",
    "    args.model,\n",
    "    num_classes=args.nb_classes,\n",
    "    drop_rate=0.0,\n",
    "    drop_path_rate=args.drop_path,\n",
    "    #attn_drop_rate=0.0,\n",
    "    drop_block_rate=None,\n",
    "    **args.__dict__\n",
    ")\n",
    "\n",
    "device = args.device\n",
    "\n",
    "# Carica i pesi del checkpoint nel modello\n",
    "checkpoint_path = \"output/checkpoint-best_850video.pth\"  \n",
    "if os.path.exists(checkpoint_path):\n",
    "    ckpt = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    if \"model\" in ckpt:\n",
    "        missing = model.load_state_dict(ckpt[\"model\"], strict=False)\n",
    "        print(f\"Checkpoint loaded. Missing keys: {missing.missing_keys}\")\n",
    "    else:\n",
    "        # Altri formati di caricamento possibili, a seconda di come hai salvato.\n",
    "        model.load_state_dict(ckpt, strict=False)\n",
    "    print(\"Checkpoint caricato correttamente.\")\n",
    "else:\n",
    "    print(\"ATTENZIONE: file checkpoint non trovato. Userai i pesi random del modello.\")\n",
    "\n",
    "\n",
    "model.to(args.device)\n",
    "model.eval()   \n",
    "\n",
    "\n",
    "all_paths, all_preds, all_labels = get_path_pred_label(model, data_loader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array(all_labels)\n",
    "y_pred  = np.array(all_preds)\n",
    "np.save(\"y_true.npy\",y_true)\n",
    "np.save(\"y_pred.npy\",y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.load(\"y_true.npy\")\n",
    "y_pred = np.load(\"y_pred.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Confusion Matrix ===\n",
      "[[419  81]\n",
      " [181 319]]\n",
      "True Positive: 319\n",
      "True Negative: 419\n",
      "False Positive: 81\n",
      "False Negative: 181\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.70      0.84      0.76       500\n",
      "         pos       0.80      0.64      0.71       500\n",
      "\n",
      "    accuracy                           0.74      1000\n",
      "   macro avg       0.75      0.74      0.74      1000\n",
      "weighted avg       0.75      0.74      0.74      1000\n",
      "\n",
      "\n",
      "=== Metriche ===\n",
      "accuracy               0.738000\n",
      "balanced_accuracy      0.738000\n",
      "precision              0.797500\n",
      "recall                 0.638000\n",
      "f1                     0.708889\n",
      "matthews_corrcoef      0.485815\n",
      "cohen_kappa            0.476000\n",
      "true_negatives       419.000000\n",
      "false_positives       81.000000\n",
      "false_negatives      181.000000\n",
      "true_positives       319.000000\n",
      "Name: metrics, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = evaluate_binary_classifier(y_true, y_pred)\n",
    "print(\"\\n=== Metriche ===\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.638"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "319/(319+181)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7975"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "319/(319+81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.738"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(419+319)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videomae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
