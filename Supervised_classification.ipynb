{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniele/anaconda2024/envs/videomae/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "from transformers import (\n",
    "    VideoMAEFeatureExtractor,\n",
    "    VideoMAEForVideoClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVideoDataset(Dataset):\n",
    "    def __init__(self, csv_path, feature_extractor, transform=None):\n",
    "        \"\"\"\n",
    "        csv_path: CSV con colonne: path, start, end, label\n",
    "        feature_extractor: es. VideoMAEFeatureExtractor di Hugging Face\n",
    "        transform: eventuali transform personalizzati\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        folder_path = row['path']   # es. '/path/part1'\n",
    "        label = row['label']\n",
    "\n",
    "        # Carica i 16 frame\n",
    "        frames = sorted(os.listdir(folder_path))\n",
    "        images_pil = []\n",
    "        for frame_file in frames:\n",
    "            frame_path = os.path.join(folder_path, frame_file)\n",
    "            img = Image.open(frame_path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            images_pil.append(img)\n",
    "\n",
    "        # Con Hugging Face, tipicamente passiamo una lista di PIL Image (o np.array)\n",
    "        # al FeatureExtractor\n",
    "        encoding = self.feature_extractor(images_pil, return_tensors=\"pt\")\n",
    "        # Ritorna un dict con chiave 'pixel_values' contenente i tensori del batch\n",
    "        # Se c'è la dimensione batch=1, di solito shape = [1, num_frames, 3, H, W]\n",
    "\n",
    "        # Restituiamo anche la label\n",
    "        encoding[\"labels\"] = torch.tensor(label, dtype=torch.long)\n",
    "        return encoding\n",
    "\n",
    "    \n",
    "\n",
    "# ESEMPIO: costruiamo dataset train e dataset val\n",
    "feature_extractor = VideoMAEFeatureExtractor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "train_data = MyVideoDataset(csv_path=\"/percorso/train.csv\", feature_extractor=feature_extractor)\n",
    "val_data   = MyVideoDataset(csv_path=\"/percorso/val.csv\",   feature_extractor=feature_extractor)\n",
    "\n",
    "# Convertiamo in Hugging Face dataset \"al volo\"\n",
    "# (volendo potresti usare la classe MyVideoDataset direttamente in un PyTorch Trainer personalizzato,\n",
    "#  ma qui mostriamo la via standard HF)\n",
    "hf_train_data = HFDataset.from_dict({\"input\": list(range(len(train_data)))})\n",
    "hf_val_data   = HFDataset.from_dict({\"input\": list(range(len(val_data)))})\n",
    "\n",
    "# Definiamo funzioni di \"mapping\" per convertire l’indice in un batch (usando MyVideoDataset)\n",
    "def collate_fn(examples):\n",
    "    # Esempio semplificato: cumuliamo i dictionary (pixel_values, labels, etc.)\n",
    "    # Dato che MyVideoDataset restituisce un dict, potremmo far passare direttamente\n",
    "    # i tensori. Oppure creiamo una custom collate.\n",
    "    # Qui usiamo la \"transform\" style di HF:\n",
    "    pixel_values = []\n",
    "    labels = []\n",
    "    for e in examples:\n",
    "        item = train_data[e[\"input\"]] if \"train\" in e[\"__index_level_0__\"] else val_data[e[\"input\"]]\n",
    "        pixel_values.append(item[\"pixel_values\"])\n",
    "        labels.append(item[\"labels\"])\n",
    "    # Concateniamo\n",
    "    pixel_values = torch.cat(pixel_values, dim=0)  # shape: [batch_size, num_frames, 3, H, W]\n",
    "    labels       = torch.stack(labels, dim=0)\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "# Riconosciamo che la HF Trainer di solito vuole una funzione .map() su dataset\n",
    "# oppure un custom collator. Ecco un custom dataset:\n",
    "\n",
    "def gen_batch(batch):\n",
    "    # batch: list of indices\n",
    "    pixel_values = []\n",
    "    labels = []\n",
    "    for index in batch[\"input\"]:\n",
    "        item = train_data[index]\n",
    "        pixel_values.append(item[\"pixel_values\"])\n",
    "        labels.append(item[\"labels\"])\n",
    "    pixel_values = torch.cat(pixel_values, dim=0)\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "\n",
    "hf_train_data.set_transform(gen_batch)\n",
    "hf_val_data.set_transform(gen_batch)\n",
    "\n",
    "model = VideoMAEForVideoClassification.from_pretrained(\n",
    "    \"MCG-NJU/videomae-base\", \n",
    "    num_labels=2  # binario\n",
    ")\n",
    "\n",
    "# Impostiamo argomenti di training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./video-checkpoints\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=1,  # dipende dalla GPU\n",
    "    per_device_eval_batch_size=1,\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=False,  # se vuoi mixed precision = True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_train_data,\n",
    "    eval_dataset=hf_val_data,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./video-checkpoints/binary-video-model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda24 (videomae)",
   "language": "python",
   "name": "videomae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
